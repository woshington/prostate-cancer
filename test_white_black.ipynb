{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from efficientnet_pytorch import model as efficientnet_model\n",
    "import albumentations\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from tqdm import tqdm\n",
    "from skimage import io as skio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_id</th>\n",
       "      <th>data_provider</th>\n",
       "      <th>isup_grade</th>\n",
       "      <th>gleason_score</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>34a98ca2d4eb1a91e428bf2112e26543</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>1</td>\n",
       "      <td>3+3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1df32b02eaa3cfad5d8c51a3e289cfc1</td>\n",
       "      <td>radboud</td>\n",
       "      <td>1</td>\n",
       "      <td>3+3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>e00a9e967dd9d4b5b23d92f7c791fbdd</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>2</td>\n",
       "      <td>3+4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1d363430fe95e7bf857cd20b5759a9cb</td>\n",
       "      <td>radboud</td>\n",
       "      <td>5</td>\n",
       "      <td>4+5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>ddd2f284a0815fb717d5ad3ee2d0d3d8</td>\n",
       "      <td>radboud</td>\n",
       "      <td>3</td>\n",
       "      <td>4+3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          image_id data_provider  isup_grade  \\\n",
       "0         1.0  34a98ca2d4eb1a91e428bf2112e26543    karolinska           1   \n",
       "1         3.0  1df32b02eaa3cfad5d8c51a3e289cfc1       radboud           1   \n",
       "2         5.0  e00a9e967dd9d4b5b23d92f7c791fbdd    karolinska           2   \n",
       "3         6.0  1d363430fe95e7bf857cd20b5759a9cb       radboud           5   \n",
       "4         7.0  ddd2f284a0815fb717d5ad3ee2d0d3d8       radboud           3   \n",
       "\n",
       "  gleason_score  fold  \n",
       "0           3+3     3  \n",
       "1           3+3     3  \n",
       "2           3+4     0  \n",
       "3           4+5     0  \n",
       "4           4+3     2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = 'dataset' # Diretório raiz dos dados\n",
    "images_dir = os.path.join(data_dir, 'tiles') # Path para o diretório das imagens\n",
    "\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_val = pd.read_csv(\"data/val.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_val = df_val.drop(columns=[\"Unnamed: 0\"])\n",
    "df_test = df_test.drop(columns=[\"Unnamed: 0\"])\n",
    "df_train_val = pd.concat([df_train, df_val])\n",
    "\n",
    "df_pen_mask = pd.read_csv(\"data/without_pen_mask.csv\")\n",
    "\n",
    "df_filtered = df_train_val[~df_train_val['image_id'].isin(df_pen_mask['image_id'])]\n",
    "\n",
    "\n",
    "\n",
    "n_folds = 5 # Número de folds da validação cruzada\n",
    "seed = 42 # Semente aleatória\n",
    "shuffle = True # Embaralha os dados\n",
    "\n",
    "batch_size = 1 # Tamanho do batch\n",
    "num_workers = 4 #N'úmero de processos paralelos que carregam os dados\n",
    "output_classes = 5 # Número de classes\n",
    "init_lr = 3e-4 # Taxa de aprendizado inicial\n",
    "warmup_factor = 10 #Fator de aquecimento para aumentar gradualmente a taxa de aprendizado no início do treinamento.\n",
    "loss_function = nn.BCEWithLogitsLoss() # Função de perda\n",
    "\n",
    "warmup_epochs = 1 #Número de épocas de warmup, durante as quais a taxa de aprendizado aumenta progressivamente.\n",
    "\n",
    "n_epochs = 30 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataframe_train = df_filtered.reset_index(drop=True)\n",
    "dataframe_train.columns = dataframe_train.columns.str.strip() # Remove espaços em branco\n",
    "\n",
    "\n",
    "stratified_k_fold = StratifiedKFold(n_folds, shuffle = shuffle, random_state=seed)\n",
    "\n",
    "dataframe_train['fold'] = -1 \n",
    "\n",
    "for i, (train_indexes, valid_indexes) in enumerate(stratified_k_fold.split(dataframe_train, dataframe_train['isup_grade'])):\n",
    "    dataframe_train.loc[valid_indexes, 'fold'] = i\n",
    "    \n",
    "dataframe_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    \"\"\"\n",
    "        Classe que implementa a arquitetura EfficientNet\n",
    "\n",
    "        Parâmetros:\n",
    "            backbone: str\n",
    "                Nome do modelo de EfficientNet a ser utilizado\n",
    "            output_dimensions: int\n",
    "                Número de neuronios na camada de saída\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dimensions):\n",
    "        super().__init__()\n",
    "        self.efficient_net = efficientnet_model.EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "        self.efficient_net.load_state_dict(\n",
    "            torch.load(\n",
    "                \"pre-trained-models/efficientnet-b0-08094119.pth\",\n",
    "                weights_only=True\n",
    "            )\n",
    "        )\n",
    "        self.fully_connected = nn.Linear(self.efficient_net._fc.in_features, output_dimensions)\n",
    "        self.efficient_net._fc = nn.Identity()\n",
    "\n",
    "    def extract(self, inputs):\n",
    "        return self.efficient_net(inputs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.extract(inputs)\n",
    "        x = self.fully_connected(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, root_dir, dataframe, transforms=None):\n",
    "        self.root_dir = root_dir \n",
    "        self.dataframe = dataframe\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataframe.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        img_id = row.image_id.strip()\n",
    "        \n",
    "        file_path = os.path.join(self.root_dir, f'{img_id}.jpg')\n",
    "        tile_image = skio.imread(file_path)\n",
    "        \n",
    "        # Substituir pixels brancos por pretos\n",
    "        white = np.array([255, 255, 255])\n",
    "        black = np.array([0, 0, 0])\n",
    "        white_pixels = np.all(tile_image == white, axis=-1)  # Identifica pixels brancos\n",
    "        tile_image[white_pixels] = black  # Substitui os pixels brancos por preto\n",
    "\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            tile_image = self.transforms(image=tile_image)['image'] \n",
    "\n",
    "        tile_image = tile_image.astype(np.float32) / 255.0 \n",
    "        tile_image = np.transpose(tile_image, (2, 0, 1))\n",
    "\n",
    "        label = np.zeros(5).astype(np.float32)\n",
    "        label[:row.isup_grade] = 1.\n",
    "        \n",
    "        return torch.tensor(tile_image, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentations\n",
    "transforms_train = albumentations.Compose([\n",
    "    albumentations.Transpose(p=0.5),\n",
    "    albumentations.VerticalFlip(p=0.5),\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(preds, targets, df):\n",
    "    accuracy = (preds == targets).mean() * 100. # Calcula a acurácia em porcentagem\n",
    "    quadraditic_weighted_kappa = cohen_kappa_score(preds, targets, weights='quadratic') # Calcula o kappa quadrático ponderado dos dados em geral\n",
    "\n",
    "    quadraditic_weighted_kappa_karolinska = cohen_kappa_score(preds[df['data_provider'] == 'karolinska'], df[df['data_provider'] == 'karolinska'].isup_grade.values, weights='quadratic')\n",
    "    quadraditic_weighted_kappa_radboud = cohen_kappa_score(preds[df['data_provider'] == 'radboud'], df[df['data_provider'] == 'radboud'].isup_grade.values, weights='quadratic')\n",
    "\n",
    "    return accuracy, quadraditic_weighted_kappa, quadraditic_weighted_kappa_karolinska, quadraditic_weighted_kappa_radboud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = PandasDataset(images_dir, df_test, transforms = None)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, dataset_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "        Função que realiza uma etapa de treinamento do modelo\n",
    "\n",
    "        Parâmetros:\n",
    "            model: Modelo a ser treinado\n",
    "            dataset_loader: DataLoader do PyTorch\n",
    "            optimizer: Otimizador a ser utilizado\n",
    "\n",
    "        Retorna:\n",
    "            train_loss: Lista com o valor do loss de treinamento\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "\n",
    "    bar_progress = tqdm(dataset_loader)\n",
    "\n",
    "    for index, (batch_data, batch_targets) in enumerate(bar_progress):\n",
    "        batch_data, batch_targets = batch_data.to(device), batch_targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zera os gradientes\n",
    "        \n",
    "        logits = model(batch_data)   # Classificação do lote de dados / Retorna o valores brutos dos neurônios (logits)\n",
    "        \n",
    "        loss = loss_function(logits, batch_targets)  # Calcula a perda de treino\n",
    "\n",
    "        loss.backward()  # Aplica o backpropagation\n",
    "\n",
    "        optimizer.step()  # Atualiza os pesos do modelo com base nos gradientes calculados durante o backpropagation\n",
    "\n",
    "        loss_np = loss.detach().cpu().numpy() # Converte o loss para numpy e remove a dependência do grafo computacional (detach)\n",
    "        train_loss.append(loss_np)\n",
    "        #Calcula a perda média suavizada, considerando as últimas 100 perdas. Isso ajuda a monitorar a evolução da perda ao longo \n",
    "        # do tempo sem ser influenciado por variações bruscas.\n",
    "        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n",
    "        \n",
    "        print(f'epoch: {epoch} batch: {(index+1)} of {len(bar_progress)} loss: {loss_np}, smooth loss: {smooth_loss}')\n",
    "        bar_progress.set_description('loss: %.5f, smooth loss: %.5f' % (loss_np, smooth_loss)) # Atualiza a barra de progresso\n",
    "        \n",
    "    return train_loss # Retorna a perda de treino\n",
    "\n",
    "\n",
    "def validation_step(model, dataset_loader, dataframe_valid, get_logits = False):\n",
    "    model.eval() # Define o modelo em modo de avaliação\n",
    "\n",
    "    validation_loss = [] # Armazena o loss de validação\n",
    "    all_logits = [] # Armazena os valores brutos dos neurônios (logits)\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    bar_progress = tqdm(dataset_loader) # Barra de progresso\n",
    "\n",
    "    with torch.no_grad(): # Desativa o cálculo de gradientes\n",
    "        for index, (batch_data, batch_targets) in enumerate(bar_progress): # Itera sobre o DataLoader \n",
    "            batch_data, batch_targets = batch_data.to(device), batch_targets.to(device) # Move os dados para o device\n",
    "\n",
    "            logits = model(batch_data) # Classificação do lote de dados / Retorna o valores brutos dos neurônios (logits)   \n",
    "\n",
    "            loss = loss_function(logits, batch_targets) # Calcula a perda entre os logits previstos e os targets\n",
    "\n",
    "            prediction = logits.sigmoid() # Aplica a função sigmoid para converter os logits em probabilidades\n",
    "            prediction = prediction.sum(1).detach().round() #  Realiza a soma do valores de saída, removendo a dependência do grafo computacional (detach)\n",
    "                                                            # e arredonda o valor para o inteiro mais próximo\n",
    "        \n",
    "            all_logits.append(logits) # Salva os logits na lista\n",
    "            preds.append(prediction) # Salva as predições na lista\n",
    "            targets.append(batch_targets.sum(1)) # realiza a soma dos targets para obter o valor do isup_grade (rotulo real) e salva na lista\n",
    "\n",
    "            validation_loss.append(loss.detach().cpu().numpy()) # Salva o loss de validação\n",
    "\n",
    "        validation_loss = np.mean(validation_loss) # Calcula a perda média de validação durante a época\n",
    "\n",
    "    # Concatena os logits, predições e targets\n",
    "    all_logits = torch.cat(all_logits).cpu().numpy()\n",
    "    preds = torch.cat(preds).cpu().numpy()\n",
    "    targets = torch.cat(targets).cpu().numpy()\n",
    "\n",
    "    # Após inferir todos os dados de validação, calcula as métricas, inclusive o kappa quadrático ponderado para cada data_provider\n",
    "    accuracy, quadraditic_weighted_kappa, quadraditic_weighted_kappa_karolinska, quadraditic_weighted_kappa_radboud = calculate_metrics(preds, targets, dataframe_valid)\n",
    "\n",
    "    # Retorna as métricas se get_logits for False, caso contrário, retorna os logits (valores brutos dos neurônios)\n",
    "    if not get_logits:\n",
    "        return {\n",
    "            \"val_loss\":validation_loss, \n",
    "            \"val_acc\":accuracy, \n",
    "            \"quadraditic_weighted_kappa\":quadraditic_weighted_kappa, \n",
    "            \"quadraditic_weighted_kappa_karolinska\":quadraditic_weighted_kappa_karolinska, \n",
    "            \"quadraditic_weighted_kappa_radboud\":quadraditic_weighted_kappa_radboud\n",
    "        }\n",
    "    else:\n",
    "        all_logits\n",
    "\n",
    "def model_checkpoint(model, best_metric, acctualy_metric, path):\n",
    "    \"\"\"\n",
    "        Função que salva o modelo\n",
    "\n",
    "        Parâmetros:\n",
    "            model: Modelo a ser salvo\n",
    "            best_metric: Melhor métrica\n",
    "            acctualy_metric: Métrica atual\n",
    "            path: Caminho para salvar o modelo\n",
    "    \"\"\"\n",
    "\n",
    "    if acctualy_metric > best_metric:\n",
    "        print(f\"Salvando o melhor modelo... {best_metric} -> {acctualy_metric}\")\n",
    "        torch.save(model.state_dict(), path)\n",
    "        best_metric = acctualy_metric\n",
    "\n",
    "    return best_metric\n",
    "\n",
    "\n",
    "def train_model(model, epochs, optimizer, scheduler, train_dataloader, valid_dataloader, valid_dataframe, path_to_save_model):\n",
    "    best_metric_criteria = 0. # Critério de melhor métrica\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f'Epoch {epoch}/{epochs}')\n",
    "\n",
    "        train_loss = training_step(model, train_dataloader, optimizer, epoch) # Realiza a etapa de treinamento\n",
    "        metrics = validation_step(model, valid_dataloader, valid_dataframe) # Realiza a etapa de validação\n",
    "\n",
    "        log_epoch = f'lr: {optimizer.param_groups[0][\"lr\"]:.7f} | Train loss: {np.mean(train_loss)} | Validation loss: {metrics[\"val_loss\"]} | Validation accuracy: {metrics[\"val_acc\"]} | QWKappa: {metrics[\"quadraditic_weighted_kappa\"]} | QWKappa Karolinska: {metrics[\"quadraditic_weighted_kappa_karolinska\"]} | QWKappa Radboud: {metrics[\"quadraditic_weighted_kappa_radboud\"]}'\n",
    "        with open('train/logs/5_fold_hsv.txt', 'a') as f:\n",
    "            f.write(log_epoch + '\\n')\n",
    "        \n",
    "        # Salva o modelo se a métrica atual for melhor que a melhor métrica / Atualmente a métrica é o kappa quadrático ponderado\n",
    "        best_metric_criteria = model_checkpoint(model, best_metric_criteria, metrics[\"quadraditic_weighted_kappa\"], path_to_save_model)\n",
    "\n",
    "        scheduler.step() # Atualiza o scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando treino do fold: 0\n",
      "train: 6774 images | validation: 1694 images\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.66925, smooth loss: 0.66925:   0%|          | 1/6774 [00:01<2:15:59,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 1 of 6774 loss: 0.6692526340484619, smooth loss: 0.6692526340484619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.65009, smooth loss: 0.65967:   0%|          | 2/6774 [00:01<1:29:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 2 of 6774 loss: 0.6500861048698425, smooth loss: 0.6596693992614746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.70455, smooth loss: 0.67463:   0%|          | 3/6774 [00:02<1:12:46,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 3 of 6774 loss: 0.7045513987541199, smooth loss: 0.6746301054954529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.73426, smooth loss: 0.68954:   0%|          | 4/6774 [00:02<1:05:04,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 4 of 6774 loss: 0.7342609167098999, smooth loss: 0.689537763595581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.68534, smooth loss: 0.68870:   0%|          | 5/6774 [00:03<1:00:54,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 5 of 6774 loss: 0.6853383183479309, smooth loss: 0.688697874546051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.68236, smooth loss: 0.68764:   0%|          | 6/6774 [00:03<58:21,  1.93it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 6 of 6774 loss: 0.6823559403419495, smooth loss: 0.687640905380249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.66673, smooth loss: 0.68465:   0%|          | 7/6774 [00:04<56:36,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 7 of 6774 loss: 0.6667254567146301, smooth loss: 0.6846529841423035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.65283, smooth loss: 0.68068:   0%|          | 8/6774 [00:04<55:28,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 8 of 6774 loss: 0.6528291702270508, smooth loss: 0.6806750297546387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.72722, smooth loss: 0.68585:   0%|          | 9/6774 [00:05<54:41,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 9 of 6774 loss: 0.7272151112556458, smooth loss: 0.6858461499214172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.64875, smooth loss: 0.68214:   0%|          | 10/6774 [00:05<54:16,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 10 of 6774 loss: 0.648746907711029, smooth loss: 0.6821362376213074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.71565, smooth loss: 0.68518:   0%|          | 11/6774 [00:05<53:52,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 11 of 6774 loss: 0.715645968914032, smooth loss: 0.6851825714111328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.72127, smooth loss: 0.68819:   0%|          | 12/6774 [00:06<53:40,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 12 of 6774 loss: 0.721272349357605, smooth loss: 0.6881900429725647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.69872, smooth loss: 0.68900:   0%|          | 13/6774 [00:06<53:27,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 13 of 6774 loss: 0.6987214088439941, smooth loss: 0.6890001893043518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.67846, smooth loss: 0.68825:   0%|          | 14/6774 [00:07<53:21,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 14 of 6774 loss: 0.6784593462944031, smooth loss: 0.6882472634315491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.70334, smooth loss: 0.68925:   0%|          | 15/6774 [00:07<53:13,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 15 of 6774 loss: 0.703341007232666, smooth loss: 0.6892535090446472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.65259, smooth loss: 0.68696:   0%|          | 16/6774 [00:08<53:08,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 16 of 6774 loss: 0.6525937914848328, smooth loss: 0.6869622468948364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.71410, smooth loss: 0.68856:   0%|          | 17/6774 [00:08<53:05,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 17 of 6774 loss: 0.7140981554985046, smooth loss: 0.6885584592819214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.71333, smooth loss: 0.68993:   0%|          | 18/6774 [00:09<53:07,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 18 of 6774 loss: 0.7133328914642334, smooth loss: 0.6899348497390747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.69730, smooth loss: 0.69032:   0%|          | 19/6774 [00:09<53:05,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 19 of 6774 loss: 0.6973032355308533, smooth loss: 0.6903226375579834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.67130, smooth loss: 0.68937:   0%|          | 20/6774 [00:10<53:01,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 20 of 6774 loss: 0.6713048815727234, smooth loss: 0.6893717050552368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.70851, smooth loss: 0.69028:   0%|          | 21/6774 [00:10<52:58,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 21 of 6774 loss: 0.7085076570510864, smooth loss: 0.690282940864563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.69797, smooth loss: 0.69063:   0%|          | 22/6774 [00:11<53:01,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 22 of 6774 loss: 0.697969377040863, smooth loss: 0.6906323432922363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.65559, smooth loss: 0.68911:   0%|          | 23/6774 [00:11<53:11,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 23 of 6774 loss: 0.6555896997451782, smooth loss: 0.6891087889671326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.65559, smooth loss: 0.68911:   0%|          | 23/6774 [00:12<59:11,  1.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m GradualWarmupScheduler(optimizer, multiplier \u001b[38;5;241m=\u001b[39m warmup_factor, total_epoch \u001b[38;5;241m=\u001b[39m warmup_epochs, after_scheduler\u001b[38;5;241m=\u001b[39mscheduler_cosine) \u001b[38;5;66;03m# Ajusta a taxa de aprendizado gradualmente durante a fase de warmup, depois utiliza o scheduler_cosine\u001b[39;00m\n\u001b[1;32m     27\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/saved_models/fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_without_pen_hsv.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe_valid_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 118\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, optimizer, scheduler, train_dataloader, valid_dataloader, valid_dataframe, path_to_save_model)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Realiza a etapa de treinamento\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m validation_step(model, valid_dataloader, valid_dataframe) \u001b[38;5;66;03m# Realiza a etapa de validação\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     log_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.7f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(train_loss)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Validation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | QWKappa: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquadraditic_weighted_kappa\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | QWKappa Karolinska: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquadraditic_weighted_kappa_karolinska\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | QWKappa Radboud: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquadraditic_weighted_kappa_radboud\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m, in \u001b[0;36mtraining_step\u001b[0;34m(model, dataset_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Aplica o backpropagation\u001b[39;00m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Atualiza os pesos do modelo com base nos gradientes calculados durante o backpropagation\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m loss_np \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;66;03m# Converte o loss para numpy e remove a dependência do grafo computacional (detach)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(loss_np)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#Calcula a perda média suavizada, considerando as últimas 100 perdas. Isso ajuda a monitorar a evolução da perda ao longo \u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# do tempo sem ser influenciado por variações bruscas.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for fold in range(n_folds):\n",
    "    print(f\"Iniciando treino do fold: {fold}\")\n",
    "    \n",
    "    train_indexes = np.where((dataframe_train['fold'] != fold))[0] # Pega os índices de treino / Todos os índices que não correspondem ao fold atual são de treino\n",
    "    valid_indexex = np.where((dataframe_train['fold'] == fold))[0] # Pega os índices de validação\n",
    "\n",
    "    dataframe_train_fold = dataframe_train.loc[train_indexes]\n",
    "    dataframe_valid_fold = dataframe_train.loc[valid_indexex]\n",
    "\n",
    "    dataset_train = PandasDataset(images_dir, dataframe_train_fold, transforms = transforms_train) # Instancia o dataset de treino\n",
    "    dataset_valid = PandasDataset(images_dir, dataframe_valid_fold, transforms = None) # Instancia o dataset de validação\n",
    "\n",
    "    print(f\"train: {len(dataset_train)} images | validation: {len(dataset_valid)} images\")\n",
    "\n",
    "    # Inicia os dataloaders de treino e validação\n",
    "    train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, sampler = RandomSampler(dataset_train), num_workers = num_workers) \n",
    "    valid_dataloader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, sampler = SequentialSampler(dataset_valid), num_workers = num_workers)\n",
    "\n",
    "    model = EfficientNet(output_dimensions = output_classes) # Instancia o modelo EfficientNet pré-treinado\n",
    "    model = model.to(device) # Move o modelo para o device (GPU se disponível)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr = init_lr / warmup_factor) # Inicializa o otimizador Adam reduzindo a taxa de aprendizado inicial durante as primeiras iterações para evitar grandes ajustes logo no inicio\n",
    "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs - warmup_epochs) # Ajusta a taxa de aprendizado de acordo com a função cosseno / vai reduzindo de forma suave, de acordo com a função cosseno.\n",
    "                                                                                                    # Os ajustes do scheduler são realizados somente após a fase de warmup\n",
    "    scheduler = GradualWarmupScheduler(optimizer, multiplier = warmup_factor, total_epoch = warmup_epochs, after_scheduler=scheduler_cosine) # Ajusta a taxa de aprendizado gradualmente durante a fase de warmup, depois utiliza o scheduler_cosine\n",
    "\n",
    "    save_path = f'train/saved_models/fold_{fold}_without_pen_hsv.pth'\n",
    "\n",
    "    train_model(model, n_epochs, optimizer, scheduler, train_dataloader, valid_dataloader, dataframe_valid_fold, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
